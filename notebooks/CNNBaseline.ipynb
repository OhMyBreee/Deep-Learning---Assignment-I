{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b4c19d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf90f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "import cv2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----------------------------\n",
    "# Transforms (same as feature extraction)\n",
    "# ----------------------------\n",
    "normalize = transforms.Normalize((0.5,), (0.5,))\n",
    "\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "normalized_transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "augment_transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop(28, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "transform_variants = {\n",
    "    \"Base\": base_transform,\n",
    "    \"Normalized\": normalized_transform,\n",
    "    \"Augmented\": augment_transform,\n",
    "    # \"Regularized\" uses same augment pipeline but turns on dropout + weight_decay\n",
    "    \"Regularized\": augment_transform\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adf2ee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Simple CNN class with configurable dropout (dropout only used in classifier)\n",
    "# ----------------------------\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, dropout_prob=0.0):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # conv blocks\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # adaptive pool to avoid hardcoding spatial dims\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        # classifier with configurable dropout\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.dropout = nn.Dropout(dropout_prob) if dropout_prob > 0 else nn.Identity()\n",
    "        self.fc1 = nn.Linear(128 * 7 * 7, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))                  # -> 32 x H x W\n",
    "        x = self.pool(F.relu(self.conv2(x)))       # -> 64 x H/2 x W/2\n",
    "        x = self.pool(F.relu(self.conv3(x)))       # -> 128 x H/4 x W/4\n",
    "        x = self.adaptive_pool(x)                  # -> 128 x 7 x 7\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)                        # dropout only active if dropout_prob>0\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b11edb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Training & evaluation helpers\n",
    "# ----------------------------\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    acc = 100.0 * correct / total\n",
    "    return avg_loss, acc\n",
    "\n",
    "def evaluate(model, loader, criterion=None):\n",
    "    model.eval()\n",
    "    preds_all, labels_all = [], []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            preds_all.extend(preds.cpu().numpy())\n",
    "            labels_all.extend(labels.cpu().numpy())\n",
    "            if criterion is not None:\n",
    "                total_loss += criterion(outputs, labels).item()\n",
    "    loss = (total_loss / len(loader)) if (criterion is not None) else None\n",
    "    acc = 100.0 * (np.array(preds_all) == np.array(labels_all)).mean()\n",
    "    return acc, loss, np.array(preds_all), np.array(labels_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78880557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Grad-CAM (uses last conv layer conv3)\n",
    "# ----------------------------\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output.detach()\n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0].detach()\n",
    "        self.target_layer.register_forward_hook(forward_hook)\n",
    "        self.target_layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    def generate(self, input_tensor, target_class=None):\n",
    "        self.model.zero_grad()\n",
    "        out = self.model(input_tensor)\n",
    "        if target_class is None:\n",
    "            target_class = out.argmax(dim=1).item()\n",
    "        one_hot = torch.zeros_like(out)\n",
    "        one_hot[0, target_class] = 1\n",
    "        out.backward(gradient=one_hot, retain_graph=True)\n",
    "\n",
    "        grads = self.gradients        # BxCxhxw\n",
    "        acts = self.activations      # BxCxhxw\n",
    "        weights = grads.mean(dim=(2,3), keepdim=True)  # BxCx1x1\n",
    "        cam = (weights * acts).sum(dim=1)              # Bxhxw\n",
    "        cam = F.relu(cam)\n",
    "        cam = cam.squeeze().cpu().numpy()\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "        return cam, target_class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ae170db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running variant: Base ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 30\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     val_acc, val_loss, _, _ \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, criterion)\n\u001b[1;32m     32\u001b[0m     history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 16\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m preds \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Main loop: training all variations properly (dropout + weight_decay only for Regularized)\n",
    "# ----------------------------\n",
    "results = {}\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for name, transform in transform_variants.items():\n",
    "    print(f\"\\n=== Running variant: {name} ===\")\n",
    "    train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    test_ds  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=1000, shuffle=False)\n",
    "\n",
    "    # regularize logic: enable dropout in model and weight decay in optimizer only for 'Regularized'\n",
    "    is_regularize = (name == \"Regularized\")\n",
    "    dropout_prob = 0.5 if is_regularize else 0.0\n",
    "\n",
    "    model = SimpleCNN(num_classes=10, dropout_prob=dropout_prob).to(device)\n",
    "\n",
    "    weight_decay = 1e-4 if is_regularize else 0.0\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=weight_decay)\n",
    "\n",
    "    # History containers\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_acc, val_loss, _, _ = evaluate(model, test_loader, criterion)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f} Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    # Final evaluation metrics (detailed)\n",
    "    test_acc, _, preds, labels = evaluate(model, test_loader)\n",
    "    prec = precision_score(labels, preds, average='macro')\n",
    "    rec  = recall_score(labels, preds, average='macro')\n",
    "    f1   = f1_score(labels, preds, average='macro')\n",
    "    print(f\"Final Test Acc: {test_acc:.2f}%  Prec: {prec:.4f} Rec: {rec:.4f} F1: {f1:.4f}\")\n",
    "    print(\"Classification report:\\n\", classification_report(labels, preds, digits=4))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f\"{name} - Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "    # Grad-CAM: visualize one example per class (2x5 grid)\n",
    "    gradcam = GradCAM(model, model.conv3)\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(12,5))\n",
    "    axes_flat = axes.flatten()\n",
    "    for i in range(10):\n",
    "        idx = np.where(np.array(test_ds.targets) == i)[0][0]\n",
    "        img, _ = test_ds[idx]\n",
    "        inp = img.unsqueeze(0).to(device)\n",
    "        inp.requires_grad_()\n",
    "        cam, cls = gradcam.generate(inp)\n",
    "        cam_resized = cv2.resize(cam, (img.shape[2], img.shape[1]))\n",
    "        ax = axes_flat[i]\n",
    "        ax.imshow(img.squeeze().cpu().numpy(), cmap='gray')\n",
    "        ax.imshow(cam_resized, cmap='jet', alpha=0.5)\n",
    "        ax.set_title(f\"Label {i}, Pred {cls}\")\n",
    "        ax.axis(\"off\")\n",
    "    plt.suptitle(f\"Grad-CAM - {name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # store results\n",
    "    results[name] = {\"history\": history, \"test_acc\": test_acc, \"prec\": prec, \"rec\": rec, \"f1\": f1, \"cm\": cm}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be473d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary bar chart (final test accuracy)\n",
    "plt.figure(figsize=(8,4))\n",
    "names = list(results.keys())\n",
    "accs  = [results[n][\"test_acc\"] for n in names]\n",
    "plt.bar(names, accs, color='skyblue')\n",
    "plt.title(\"Final Test Accuracy per Variation\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
